{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este notebook crearemos el conjunto de entrenamiento y entrenaremos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "import random\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bajamos el modelo general para español de tamaño medio.\n",
    "Deshabilitamos el NER a ese modelo y añadimos para ese cometido nuestras reglas creadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download es_core_news_md (con esto nos bajamos el modelo)\n",
    "with open(\"./data/antecedentes_patterns_CIE10.jsonl\", encoding=\"utf8\") as f:\n",
    "    patterns = json.loads(f.read())\n",
    "    \n",
    "nlp = spacy.load(\"es_core_news_md\", disable=[\"ner\"]) \n",
    "ruler = EntityRuler(nlp, overwrite_ents=True, validate=True)\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos los componentes de nuestro pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'entity_ruler']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para cargar y salvar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)\n",
    "\n",
    "def save_data(file, data):\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para crear conjunto de entrenamiento.\n",
    "\n",
    "El formato de los datos de entremamiento debe ser así: **TRAIN_DATA = [(text, {\"entities\": [(start, end, label)]})]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train(model, text):\n",
    "    doc = nlp(text)\n",
    "    results = []\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    if len(entities) > 0:\n",
    "        results = [text, {\"entities\": entities}]\n",
    "        return (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear el set de entrenamiento utilizando el dataset de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"./data/train/text_files/*.txt\")\n",
    "             \n",
    "with open(\"./data/train/allfiles.txt\", \"w\", encoding=\"utf-8\" ) as result:\n",
    "    for file in files:\n",
    "        for line in open( file, \"r\", encoding=\"utf-8\" ):\n",
    "            result.write( line )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el conjunto de entrenamiento y lo guardamos en un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = []\n",
    "with open (\"./data/train/allfiles.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    text_to_train = f.read()\n",
    "    #print (text)\n",
    "    segments = text_to_train.split(\"\\n\")\n",
    "    for segment in segments:\n",
    "        results = create_train(nlp, segment)\n",
    "        if results != None:\n",
    "            TRAIN_DATA.append(results)\n",
    "\n",
    "save_data(\"./data/antecedentes_training_data.jsonl\", TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos los antecedentes entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de antecedente en datos de entrenamiento: 642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (f\"Número de antecedente en datos de entrenamiento: {len(TRAIN_DATA)}\\n\")\n",
    "\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp(text)\n",
    "    #print('Antecedente entrenado', [(ent.text) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, iterations):\n",
    "    TRAIN_DATA = data\n",
    "    \n",
    "    # Creamos un modelo de spacy vacio\n",
    "    #nlp = spacy.blank(\"en\") \n",
    "    #nlp = Spanish()\n",
    "    \n",
    "    # Añadimos la componente ner a la pipeline sino no está\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    \n",
    "    # Añadimos todas las antecedentes del conjunto de entrenamiento al modelo\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "            \n",
    "    # Eliminamos el efecto del training en otros pipes \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    \n",
    "    # Empezamos a entrenar\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            #print (\"Starting iteration \" + str(itn))\n",
    "            # Mezcla los datos de entrenamiento\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # Crea lotes con los ejemplos e itera sobre ellos\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # Actualiza el modelo\n",
    "                nlp.update(\n",
    "                    texts,  \n",
    "                    annotations,  \n",
    "                    drop=0.1,  # dropout - hace mas dificil que se momoricen los datos de entrenamiento para evitar el sobrenetrenamiento\n",
    "                    losses=losses,\n",
    "                )\n",
    "            #print (losses)\n",
    "            print(f\"Losses at iteration {itn} - {dt.datetime.now()} {losses}\")\n",
    "    return (nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de entrenamiento, entrenamos el modelo en 30 iteraciones y lo guardamos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0 - 2021-07-13 00:02:50.398382 {'ner': 7274.298055671835}\n",
      "Losses at iteration 1 - 2021-07-13 00:03:10.963561 {'ner': 5407.355671226091}\n",
      "Losses at iteration 2 - 2021-07-13 00:03:30.842077 {'ner': 2287.9541340209857}\n",
      "Losses at iteration 3 - 2021-07-13 00:03:51.176389 {'ner': 1896.5656933267956}\n",
      "Losses at iteration 4 - 2021-07-13 00:04:11.968789 {'ner': 1497.9757393154068}\n",
      "Losses at iteration 5 - 2021-07-13 00:04:32.278157 {'ner': 2239.0680259096653}\n",
      "Losses at iteration 6 - 2021-07-13 00:04:52.476827 {'ner': 866.7329936359725}\n",
      "Losses at iteration 7 - 2021-07-13 00:05:13.185722 {'ner': 709.3802187247344}\n",
      "Losses at iteration 8 - 2021-07-13 00:05:33.925939 {'ner': 598.5033197738369}\n",
      "Losses at iteration 9 - 2021-07-13 00:05:54.740442 {'ner': 515.2521536570903}\n",
      "Losses at iteration 10 - 2021-07-13 00:06:16.090055 {'ner': 537.0521909140771}\n",
      "Losses at iteration 11 - 2021-07-13 00:06:37.970014 {'ner': 438.97966394766104}\n",
      "Losses at iteration 12 - 2021-07-13 00:06:59.142072 {'ner': 285.07706072818416}\n",
      "Losses at iteration 13 - 2021-07-13 00:07:20.756615 {'ner': 326.98549681013196}\n",
      "Losses at iteration 14 - 2021-07-13 00:07:42.927005 {'ner': 247.8910221726512}\n",
      "Losses at iteration 15 - 2021-07-13 00:08:05.648914 {'ner': 187.22081602672657}\n",
      "Losses at iteration 16 - 2021-07-13 00:08:29.004254 {'ner': 193.83800885460818}\n",
      "Losses at iteration 17 - 2021-07-13 00:08:52.873113 {'ner': 182.10771320581745}\n",
      "Losses at iteration 18 - 2021-07-13 00:09:16.677305 {'ner': 211.98692163368267}\n",
      "Losses at iteration 19 - 2021-07-13 00:09:40.156187 {'ner': 153.4470522623517}\n",
      "Losses at iteration 20 - 2021-07-13 00:10:04.010201 {'ner': 210.63771599182277}\n",
      "Losses at iteration 21 - 2021-07-13 00:10:27.544947 {'ner': 227.1901036272358}\n",
      "Losses at iteration 22 - 2021-07-13 00:10:51.573027 {'ner': 212.71557302914803}\n",
      "Losses at iteration 23 - 2021-07-13 00:11:15.460838 {'ner': 250.33958597282006}\n",
      "Losses at iteration 24 - 2021-07-13 00:11:39.260515 {'ner': 93.15077946652967}\n",
      "Losses at iteration 25 - 2021-07-13 00:12:02.852115 {'ner': 138.85422182067325}\n",
      "Losses at iteration 26 - 2021-07-13 00:12:26.953660 {'ner': 165.77057190119828}\n",
      "Losses at iteration 27 - 2021-07-13 00:12:50.554235 {'ner': 169.73103266032666}\n",
      "Losses at iteration 28 - 2021-07-13 00:13:14.379822 {'ner': 300.8316680415299}\n",
      "Losses at iteration 29 - 2021-07-13 00:13:38.305520 {'ner': 157.21763068408265}\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = load_data(\"./data/antecedentes_training_data.jsonl\")\n",
    "nlp = train_model(TRAIN_DATA, 30)\n",
    "nlp.to_disk(\"./models/model_antecedentes_ner_es_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'entity_ruler', 'ner']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
